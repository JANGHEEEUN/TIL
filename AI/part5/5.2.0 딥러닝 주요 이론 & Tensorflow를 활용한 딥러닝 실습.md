---

---

# 딥러닝 주요 이론 & Tensorflow를 활용한 딥러닝 실습

## 머신 러닝 vs 딥러닝

### What is Tensorflow

> 1. 구글 브레인팀이 2015년 오픈소스로 공개
>
> 2. Python API 제공 / 실제 코드 실행 환경은 C/C++
>
> 3. GPU 활용 가능 (GPU에서 일반 연산을 수행하게 하는 CUDA 확장 기능 사용)

- 수학계산을 위해 만들어짐

  

공부 링크: https://www.tensorflow.org

---



### Why tensorflow

- 딥러닝을 빨리 할 수 있는 이유 = C언어 기반

- 프론트엔드 인터페이스로 다양한 언어 지원 >> **JS 지원**이 중요한 맥락: Tensorflow JS

  다양한 컴퓨터 환경에서 크롬만 있으면 자바 스크립트로 만든 딥러닝 모델에서 동일한 결과를 얻을 수 있음

- Tensorflow Light: 가벼운 사용 But 정확도가 매우 떨어짐

  *htttp://j.mp/2PVh0ZG*

- Tensorboard를 사용해서 시각화나 기록을 볼 수 있음

- tf.Saver: 게임처럼 모델 학습 단계 중 원하는 시점마다 저장 가능

  Keras 함수가 생기면서 tf.Saver보다 더 쉬운 방법이 많이 생김

- Tensorflow 흐름

  - Tensorflow 2.0 (in 2019)

    1.x와 차이점: 1.x는 전문 개발자들을 염두, but 2.x는 일반 사용자들을 대상으로 개발됨*

---



### Tensorflow basic

**tensor**

- tensor: Scalar + Vector + Matrix(행렬)

  > 벡터의 확장 개념 

<img src="D:\장희은\TIL\AI\image/image-20191230171009305.png" style="zoom:50%;" /> 



**import** 

```python
import tensorflow as tf
import numpy as np
import pandas as pd
```



**TF 실행을 위한 2단계**

> 1. Building a tensorflow graph: Tensor들 사이의 연산 관계를 계산 그래프로 정의 & 선언
> 2. Executing the tensorflow graph: 계산 그래프에 정의된 연산을 tf.Session을 통해 실제로 실행

- 2단계 없이 진행할 경우, tf 연산이 수행되지 않음

  ```python
  a = tf.add(3,5)
  print(a)
  
  #결과: Tensor("Add:0", shape=(), dtype=int32)
  ```

- 2단계인 tf.Session으로 출력

  ```python
  a = tf.add(3,5)
  sess = tf.Session()
  print(sess.run(a))
  
  sess.close()
  
  #결과: 8
  ```



**sess.close()**

tf.SSession()은 사용 후 닫아져야 함

하지만 아래와 같이 with ~ as로 사용한다면 수동으로 닫을 필요 없음

```python
a = tf.add(3,5)
with tf.Session() as sess:
    print(sess.run(a))

# sess.close()
```

**계산 저장**

tf의 장점: 아래의 코드에서 우리는 op3만 실행하고 op1,2는 실행하지 않음. 하지만 op3 실행만으로도 결과를 얻을 수 있음

```python
x = 2
y = 3
op1 = tf.add(x,y)
op2 = tf.multiply(x,y)
op3 = tf.pow(op2, op1)

with tf.Session() as sess:
    op3 = sess.run(op3)
    #만약 여러 tensor를 동시에 실행하고 싶으면 list로 전달
    # op3, unsless = sess.run([op3, useless])
    print(op3)

# tensorflow 장점: op1.2를 실행하지 않아도 연결된 op3 실행만으로도 결과 얻을 수 있음
```

---



### TensorFlow basic - Linear Regression

> 1. 데이터 준비
> 2. 모델 빌드
> 3. 표준 설정
> 4. 모델 학습
> 5. 결과 시각화



**1. Prepare the data**

데이터: 집값 예측

```python
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import datasets

x_data = datasets.load_boston().data[:, 12]
y_data = datasets.load_boston().target
df = pd.DataFrame([x_data, y_data]).transpose()
df.head()
```



**2. Build the model**

```python
w = tf.Variable(tf.random_normal([1]))
b = tf.Variable(tf.random_normal([1]))
#random_normal = parameter 쎄타에 대한 초기화

y_predicted = w * x_data + b #모델
```



**3. Set the criterion: Cost function & Gradient Descent method**

```python
loss = tf.reduce_mean(tf.square(y_predicted - y_data))
#tf.square: 제곱함수<< ()안의 숫자를 통으로 제곱 != pow << pow는 (a,b) a를 b제곱
#reduce_mean:
optimizer = tf.train.GradientDescentOptimizer(0.001)
#0.001: 하이퍼 파라미터 러닝 레이트
train = optimizer.minimize(loss)
#loss와 optimizer 결합
```



**4. Train the model**

```python
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer()) #tf.variables 초기화
    
    for step in range(10000): #10000 = 에폭 개념
        sess.run(train) # 실제로 Gradient Descent가 실행되는 코드
        if step % 1000 == 0:
            print('Step {}: w {} b {}'.format(step, sess.run(w), sess.run(b)))
            print('loss {}'.format(sess.run(loss)))
            print()
    
    w_out, b_out = sess.run([w,b])
```



**5. Visualize the result**

```python
plt.figure(figsize = (10,10))
plt.plot(x_data, y_data, 'bo', label = 'Real data')
#bo = 원
plt.plot(x_data, x_data * w_out + b_out, 'ro', label = 'Prediction')
# 예측값
plt.legend()
plt.show()
```

<img src="C:\Users\JHE\AppData\Roaming\Typora\typora-user-images\image-20191230190049849.png" alt="image-20191230190049849" style="zoom:50%;" /> 

---



### Tensorflow basic - Regression with Neural Network

> 1. 데이터 준비 - Linear Regression과 동일
> 2. 모델 빌드
> 3. 표준 설정
> 4. 모델 학습
> 5. 결과 시각화



**2. Build the model**

```python
_x_data = tf.reshape(x_data, [len(x_data), 1])
#x_data를 (x_data 행의 길이,1)인 행렬로 변환

W = tf.Variable(tf.random_normal([1,5], dtype = tf.float64))
W_out = tf.Variable(tf.random_normal([5,1], dtype = tf.float64))
# W의 5와 W_out의 5는 같음 >> 행렬 곱에서 W의 행과 W_out의 열이 같아야 가능

#matmul: 행렬곱
hidden = tf.nn.sigmoid(tf.matmul(_x_data,W))
output = tf.matmul(hidden, W_out)
```



**3. Set the criterion: Cost functioin & Gradient Descent method**

```python
loss = tf.reduce_mean(tf.square(output - y_data))
optimizer = tf.train.GradientDescentOptimizer(0.001)
train = optimizer.minimize(loss)
```



**4. Train the model**

```python
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer()) #tf.variables 초기화
    
    for step in range(50000): 
        sess.run(train)
        if step % 5000 == 0:
            print('Step {} || loss : {}'.format(step, sess.run(loss)))
            
    output = sess.run(output)
```



**5. Visualize the result**

```python
plt.figure(figsize = (10,10))
plt.plot(x_data, y_data, 'bo', label = 'Real data')
plt.plot(x_data, output, 'ro', label = 'Prediction')
plt.legend()
plt.show()
```

<img src="C:\Users\JHE\AppData\Roaming\Typora\typora-user-images\image-20191230192302108.png" alt="image-20191230192302108" style="zoom:50%;" /> 

결과가 예상 밖으로 나옴 



**2-(2). Build the model - 2 Hidden layers**

위의 결과를 원하는대로 얻기 위해 기존의 1개였던 Hidden Layer를 2개로 늘려줌

```python
_x_data = tf.reshape(x_data, [len(x_data), 1])
_y_data = tf.reshape(y_data, [len(y_data), 1])
# 2에서는 _x_data만 생성

W1 = tf.Variable(tf.random_normal([1, 5], dtype=tf.float64)) 
W2 = tf.Variable(tf.random_normal([5, 10], dtype=tf.float64)) 
W_out = tf.Variable(tf.random_normal([10, 1], dtype=tf.float64))

#2에서는 sigmoid. 2-(2)에서는 elu
hidden1 = tf.nn.elu(tf.matmul(_x_data, W1))
hidden2 = tf.nn.elu(tf.matmul(hidden1, W2))
output = tf.matmul(hidden2, W_out)
```

- `tf.nn.elu` vs `tf.nn.sigmoid`



**3-(2). Set the criterion**

```python
loss = tf.losses.mean_squared_error(output, _y_data) 
# 3에서는 tf.reduce_mean(tf.square(output - _y_data))
# 이렇게 되면 output: (506,1) y: (506,506) => 행렬곱 불가
optimizer = tf.train.AdamOptimizer(0.001)
train = optimizer.minimize(loss)
```



> 4. Train the model - 동일



**5-(2). Visualize the result**

<img src="C:\Users\JHE\AppData\Roaming\Typora\typora-user-images\image-20191230193328213.png" alt="image-20191230193328213" style="zoom:50%;" /> 



**Summary**

> Deep neural network model = Network (Neural network architecture)+ Cost Function(Criterion) + Optimizer(Gradient descent method)

![image-20191230193535557](C:\Users\JHE\AppData\Roaming\Typora\typora-user-images\image-20191230193535557.png)'



---



### Tensorflow basic - Placeholders

> placeholders: 계산 그래프를 실행(sess.run)할 때 사용자가 실제 데이터 (Train/Test/Unseen, etc.)를 흘려보낼 수 있는 통로

정의 단계에서 무심코 썼던  x,y 부분은 밖에서 들어오는 값으로 placeholders.

우리가 x,y를 정의할 수 없음 -> 실제로 x와 y에 뭐가 흘러들어갈지 알수 없음 

아래에서 x_data

<img src="C:\Users\JHE\AppData\Roaming\Typora\typora-user-images\image-20191230134557309.png" alt="image-20191230134557309" style="zoom:67%;" /> 



**tf.placeholder()**

> 1. 실제 Data가 담길 일종의 접시
> 2. 용도에 따라 Data type과 Shape을 설정하고 선언해 사용
>
> <img src="C:\Users\JHE\AppData\Roaming\Typora\typora-user-images\image-20191230204214531.png" alt="image-20191230204214531" style="zoom:67%;" /> 

- shape이 좀 민감 

  None: 데이터가 int이기만 하면 접시에 담을 수 있음

  (500,10): int인 데이터 중 500행 10열이여야만 받아들일 수 있음



**feed_dict**

> 1. sess.run() 함수에게 전달하는 Placeholder와 실제 Data의 쌍(Dictionary)
> 2. Key에 해당하는 Placeholder는 고정, 실행 단계에서 흘려보낼 Data만 Value에 입력

참고 자료: Tensorflow_main(problem)/1-3.Regression.ipynb

![image-20191230135402198](C:\Users\JHE\AppData\Roaming\Typora\typora-user-images\image-20191230135402198.png) 

---



### TensorFlow basic - Tutorial for Classification(Basic)

**MNIST**

수업 자료: Tensorflow_main(problem)/2-1.Classification.ipynb

- MNIST 손글씨 숫자 데이터

  ![image-20191230135549487](C:\Users\JHE\AppData\Roaming\Typora\typora-user-images\image-20191230135549487.png) 

  가로 세로가 28px

  1픽셀은 0~1사이의 숫자를 갖으며 0은 black, 1은 white로 가정

  28x28 -> 1x 784의 데이터로 만들 수 있음



**Import**





**Dropout**

수업 자료: Tensorflow_main(problem)/2-2.Classification&Dropout.ipynb

#training 단계와 test 단계 때 다른 것을 넣어주려고 넣는 것

껍데기 함수처럼 사용

---



### TensorFlow basic - Tutorial for Classification(adv.)

수업 자료: Tensorflow_main(problem)/3-1. (extra) Classification & Adv (MNIST, 2Layer+he_init+dropout).ipynb



**tf.nn vs tf.layer vs tf.contrib**

+ tf.nn

  dropout 有

  레고 2개 짜리. 밑으로 파고 내려가서 가장 안에 있는 부분까지 고칠 때 사용

- tf.layer

  dropout 有

  tf.layer은 nn보다 편하게 레이어를 만들도록 개발됨

- tf.contrib

  실험적인 함수들이 모여있는 그룹 >> 없어지는 추세



수업 자료: Tensorflow_main(problem)/3-2. (extra) Classification & Adv (MNIST, 2Layer+BN).ipynb

성능 개선을 위해 꼭 사용해줘야 하는 코드

```
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    optimizer = tf.train.AdamOptimizer(1e-3).minimize(cost)    
```

---



수업 자료: Tensorflow_main(problem)/3-3. (extra) Classification & Adv (MNIST, Keras Basic & AutoKeras).ipynb

---



수업 자료: Tensorflow_main(problem)/3-4. (extra) Classification & Adv (MNIST, 2Layer+BN, using Keras).ipynb

가장 쉬우면서 betch nom까지 쓴 코드

- mode

```python
model = models.load_model('mnist_2layer_bn.h5')
```

![image-20191230162158949](C:\Users\JHE\AppData\Roaming\Typora\typora-user-images\image-20191230162158949.png)  

---



수업 자료: Tensorflow_main(problem)/ 4-3. (Appendix) Saving & Loading trained models (using Keras).ipynb

tf모델을 저장하고 불러오는 코드

---



수업 자료: Tensorflow_main(problem)/ 4-1. (Appendix) TensorFlow Basic (Basic+Variable+LinearRegression).ipynb

tf 1버전에서 기초적인 코드



