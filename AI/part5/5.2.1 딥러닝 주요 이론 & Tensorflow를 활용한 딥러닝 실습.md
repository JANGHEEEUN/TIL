# 딥러닝 주요 이론 & Tensorflow를 활용한 딥러닝 실습

### Tensorflow basic - Placeholders

> placeholders: 계산 그래프를 실행(sess.run)할 때 사용자가 실제 데이터 (Train/Test/Unseen, etc.)를 흘려보낼 수 있는 통로

정의 단계에서 무심코 썼던  x,y 부분은 밖에서 들어오는 값으로 placeholders.

우리가 x,y를 정의할 수 없음 -> 실제로 x와 y에 뭐가 흘러들어갈지 알수 없음 

아래에서 x_data

<img src="C:\Users\JHE\AppData\Roaming\Typora\typora-user-images\image-20191230134557309.png" alt="image-20191230134557309" style="zoom:67%;" /> 



**tf.placeholder()**

> 1. 실제 Data가 담길 일종의 접시
> 2. 용도에 따라 Data type과 Shape을 설정하고 선언해 사용
>
> <img src="C:\Users\JHE\AppData\Roaming\Typora\typora-user-images\image-20191230204214531.png" alt="image-20191230204214531" style="zoom:67%;" /> 

- shape이 좀 민감 

  None: 데이터가 int이기만 하면 접시에 담을 수 있음

  (500,10): int인 데이터 중 500행 10열이여야만 받아들일 수 있음



**feed_dict**

> 1. sess.run() 함수에게 전달하는 Placeholder와 실제 Data의 쌍(Dictionary)
> 2. Key에 해당하는 Placeholder는 고정, 실행 단계에서 흘려보낼 Data만 Value에 입력

참고 자료: Tensorflow_main(problem)/1-3.Regression.ipynb

![image-20191230135402198](C:\Users\JHE\AppData\Roaming\Typora\typora-user-images\image-20191230135402198.png) 

---



### TensorFlow basic - Tutorial for Classification(Basic)

**MNIST**

*수업 자료: Tensorflow_main(problem)/2-1.Classification.ipynb*

- MNIST 손글씨 숫자 데이터

  ![image-20191230135549487](C:\Users\JHE\AppData\Roaming\Typora\typora-user-images\image-20191230135549487.png) 

  가로 세로가 28px

  1픽셀은 0~1사이의 숫자를 갖으며 0은 black, 1은 white로 가정

  28x28 -> 1x 784의 데이터로 만들 수 있음



**Import**

```python
import tensorflow as tf
tf.logging.set_verbosity(tf.logging.ERROR)

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' 
# https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information
```



**1. Prepare & Check the data**

```python
# mnist -> train / test / validation
# mnist 안쪽에 데이터가 3개로 나눠져 있음
# mnist.train -> 'images'(x_data) 'labels'(y_data), 'num_examples'
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("./mnist/data/", one_hot=True)

#train 데이터 타입 체크
type(mnist.train.images) #결과: numpy.ndarray

#train 데이터 shape 체크
mnist.train.images.shape #결과: (55000, 784)

#train 데이터의 y데이터 체크 <- training data의 정답 데이터
mnist.train.labels.shape  #결과: (55000, 10)

#panda로 y데이터 살펴보기
import pandas as pd 

df = pd.DataFrame(mnist.train.labels) # to pd.DataFrame
df.head()
```



**2. Build the model**

```python
# 데이터가 흘러들어올 접시(placeholder) 만들기 
# 이제는 tf.run 돌리기 전까지 xdata ydata 건들이지 않음
X = tf.placeholder(tf.float32, [None, 784]) 
# [# of batch data, # of features(columns) == 총 784개의 열]
Y = tf.placeholder(tf.float32, [None, 10]) 
# 0~9 == 총 10개의 열

# 모든 Parameter Theta는 Variable로 선언
# Weight의 열수(W1 - 256)는 Weight가 꽂힐 hidden Layer(L1)의 노드수
W1 = tf.Variable(tf.random_normal([784, 256], stddev=0.01))
L1 = tf.nn.relu(tf.matmul(X, W1))

W2 = tf.Variable(tf.random_normal([256, 256], stddev=0.01))
L2 = tf.nn.relu(tf.matmul(L1, W2))

W3 = tf.Variable(tf.random_normal([256, 10], stddev=0.01))
model = tf.matmul(L2, W3) # 마지막 층도 행렬곱까지만 진행
```



**3. Set the criterion**

```python
# cost = tf.losses.mean_squared_error(Y, model) # for Regression

cost = tf.losses.softmax_cross_entropy(Y, model) 
#분류를 위해 softmax 후에 cross-entropy 진행
#softmax_cross_entropy: 모델이 낸 예측값에 대해 softmax 씌워주고, 알아서 정답이랑 같이 정리

optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)
# Select optimizer & connect with cost function (recommended start : "Adam")
```



**4. Train the model**

```python
init = tf.global_variables_initializer() 
# Initialize all global variables (Parameter Theta)
sess = tf.Session()
sess.run(init)
```



** **



**Dropout**

수업 자료: Tensorflow_main(problem)/2-2.Classification&Dropout.ipynb

#training 단계와 test 단계 때 다른 것을 넣어주려고 넣는 것

껍데기 함수처럼 사용

---



### TensorFlow basic - Tutorial for Classification(adv.)

수업 자료: Tensorflow_main(problem)/3-1. (extra) Classification & Adv (MNIST, 2Layer+he_init+dropout).ipynb



**tf.nn vs tf.layer vs tf.contrib**

+ tf.nn

  dropout 有

  레고 2개 짜리. 밑으로 파고 내려가서 가장 안에 있는 부분까지 고칠 때 사용

- tf.layer

  dropout 有

  tf.layer은 nn보다 편하게 레이어를 만들도록 개발됨

- tf.contrib

  실험적인 함수들이 모여있는 그룹 >> 없어지는 추세



수업 자료: Tensorflow_main(problem)/3-2. (extra) Classification & Adv (MNIST, 2Layer+BN).ipynb

성능 개선을 위해 꼭 사용해줘야 하는 코드

```
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    optimizer = tf.train.AdamOptimizer(1e-3).minimize(cost)    
```

---



수업 자료: Tensorflow_main(problem)/3-3. (extra) Classification & Adv (MNIST, Keras Basic & AutoKeras).ipynb

---



수업 자료: Tensorflow_main(problem)/3-4. (extra) Classification & Adv (MNIST, 2Layer+BN, using Keras).ipynb

가장 쉬우면서 betch nom까지 쓴 코드

- mode

```python
model = models.load_model('mnist_2layer_bn.h5')
```

![image-20191230162158949](C:\Users\JHE\AppData\Roaming\Typora\typora-user-images\image-20191230162158949.png)  

---



수업 자료: Tensorflow_main(problem)/ 4-3. (Appendix) Saving & Loading trained models (using Keras).ipynb

tf모델을 저장하고 불러오는 코드

---



수업 자료: Tensorflow_main(problem)/ 4-1. (Appendix) TensorFlow Basic (Basic+Variable+LinearRegression).ipynb

tf 1버전에서 기초적인 코드



